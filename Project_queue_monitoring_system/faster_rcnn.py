# -*- coding: utf-8 -*-
"""Faster_RCNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1I1aGEW1WAwJp_D1TqNs_JarAW81jXDD1
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# !pip install nbimporter
import os
import importlib
# Import code from other_notebook.ipynb
os.chdir('/content/drive/MyDrive/Colab Notebooks/Project/')
# %run VGG_net.ipynb
# from Classifier import VGGFeature

!python -m pip install pyyaml==5.1
import sys, os, distutils.core
# Note: This is a faster way to install detectron2 in Colab, but it does not include all functionalities (e.g. compiled operators).
# See https://detectron2.readthedocs.io/tutorials/install.html for full installation instructions
# !git clone 'https://github.com/facebookresearch/detectron2'
dist = distutils.core.run_setup("./detectron2/setup.py")
!python -m pip install {' '.join([f"'{x}'" for x in dist.install_requires])}
sys.path.insert(0, os.path.abspath('./detectron2'))

# Properly install detectron2. (Please do not install twice in both ways)
# !python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'

import torch, detectron2
!nvcc --version
TORCH_VERSION = ".".join(torch.__version__.split(".")[:2])
CUDA_VERSION = torch.__version__.split("+")[-1]
print("torch: ", TORCH_VERSION, "; cuda: ", CUDA_VERSION)
print("detectron2:", detectron2.__version__)

# Some basic setup:
# Setup detectron2 logger
import detectron2
from detectron2.utils.logger import setup_logger
setup_logger()

# import some common libraries
import numpy as np
import os, json, cv2, random
from google.colab.patches import cv2_imshow

# import some common detectron2 utilities
from detectron2 import model_zoo
from detectron2.engine import DefaultPredictor
from detectron2.config import get_cfg
from detectron2.utils.visualizer import Visualizer
from detectron2.data import MetadataCatalog, DatasetCatalog

# !wget http://images.cocodataset.org/val2017/000000439715.jpg -q -O input.jpg
im = cv2.imread("/content/drive/MyDrive/Colab Notebooks/Project/Queue counting and detection.v4i.coco/valid/86ef593758609eab4618d7c96f71f47_jpg.rf.d68053f5c907080abeb4d5cf832976f7.jpg")
cv2_imshow(im)

cfg = get_cfg()
# add project-specific config (e.g., TensorMask) here if you're not running a model in detectron2's core library
cfg.merge_from_file(model_zoo.get_config_file("COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml"))
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model
# Find a model from detectron2's model zoo. You can use the https://dl.fbaipublicfiles... url as well
cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml")
predictor = DefaultPredictor(cfg)
outputs = predictor(im)

# We can use `Visualizer` to draw the predictions on the image.
v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=0.8)
out = v.draw_instance_predictions(outputs["instances"].to("cpu"))
cv2_imshow(out.get_image()[:, :, ::-1])

import os
import cv2
import matplotlib.pyplot as plt
from PIL import Image
import numpy as np
import torchvision.transforms as transforms

norm_mean = [0.485, 0.456, 0.406]
norm_std = [0.229, 0.224, 0.225]

inference_transform = transforms.Compose([
    transforms.Resize(256),
    transforms.ToTensor(),
    transforms.Normalize(norm_mean, norm_std),
])

def img_transform(img_rgb, transform=None):
    """
    transform images
    :param img_rgb: PIL Image
    :param transform: torchvision.transform
    :return: tensor
    """

    if transform is None:
        raise ValueError("there is no transform")

    img_t = transform(Image.fromarray(img_rgb))
    return img_t

class Sample:
    def __init__(self, idx=0, fname='', img=None, feat=None, label=None):
        self.idx = idx
        self.fname = fname
        self.img = img
        self.torch_img=img
        self.vgg_feat = feat
        self.label = label
        self.pred = None
# # fig, ax = plt.subplots()
samples=[]
labels={0:'not_queue',1:'queuing',2:'serving'}
cat={'not_queue':0, 'queuing':1, 'serving':2}

from detectron2.data.datasets import register_coco_instances
register_coco_instances("train", {}, "/content/drive/MyDrive/Colab Notebooks/Project/Queue counting and detection.v4i.coco/train/_annotations.coco.json", "/content/drive/MyDrive/Colab Notebooks/Project/Queue counting and detection.v4i.coco/train")
register_coco_instances("valid", {}, "/content/drive/MyDrive/Colab Notebooks/Project/Queue counting and detection.v4i.coco/valid/_annotations.coco.json", "/content/drive/MyDrive/Colab Notebooks/Project/Queue counting and detection.v4i.coco/valid")
train_metadata = MetadataCatalog.get("train")
train_dataset_dicts = DatasetCatalog.get("train")
valid_metadata = MetadataCatalog.get("valid")
valid_dataset_dicts = DatasetCatalog.get("valid")
valid_metadata.thing_classes.pop(0)
train_metadata.thing_classes.pop(0)



outputs["instances"].pred_boxes

import cv2
from detectron2 import model_zoo
from detectron2.engine import DefaultPredictor
from detectron2.config import get_cfg

def detect_and_crop(im,outputs):
    # Perform object detection
    outputs = predictor(im)
    boxes = outputs["instances"].pred_boxes
    cropped_images = []
    class_indices = outputs["instances"].pred_classes == 0
    # filtered_boxes = outputs["instances"].pred_boxes[class_indices]
    # filtered_scores = outputs["instances"].scores[class_indices]
    for i, box in enumerate(boxes):
      if(outputs["instances"].pred_classes[i]!=0):
        continue
      x, y, w, h = box
      cropped_image = im[int(y):int(y+h), int(x):int(x+w)]
      cropped_images.append(cropped_image)

    return cropped_images,class_indices

from sklearn import svm
import joblib
# Load the pretrained model state dictionary
pretrained_model_path = "/content/drive/MyDrive/Colab Notebooks/Project/output_model/3_class_vgg_extractor.pt"
vgg_model = torch.load(pretrained_model_path)
pretrained_model_path = "/content/drive/MyDrive/Colab Notebooks/Project/output_model/3_class_SVM_model.pkl"
svm_model = joblib.load(pretrained_model_path)

from detectron2.utils.visualizer import ColorMode
dataset_dicts = valid_dataset_dicts

for d in random.sample(dataset_dicts, 4):
    samples=[]
    im = cv2.imread(d["file_name"])
    outputs = predictor(im)  # format is documented at https://detectron2.readthedocs.io/tutorials/models.html#model-output-format
    v = Visualizer(im[:, :, ::-1],
                   metadata=train_metadata,
                   scale=0.5,
                   instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels. This option is only available for segmentation models
    )
    cropped,indicies=detect_and_crop(im,outputs)
    for cropped_img in cropped:
      samples.append(Sample(img=img_transform(cv2.resize(cropped_img,(200,300)), inference_transform)))
    train_feats = [sample.torch_img.numpy() for sample in samples]
    train_feats = torch.tensor(train_feats).to(torch.float32)
    train_x = torch.reshape(train_feats,[train_feats.shape[0],3,384,256])
    result,conv=vgg_model(train_x.cuda())
    predictions = svm_model.predict(conv.detach().cpu())
    outputs["instances"].set("pred_classes",torch.tensor(predictions))
    outputs["instances"].set("pred_boxes",outputs["instances"].pred_boxes[indicies])
    outputs["instances"].set("scores",outputs["instances"].scores[indicies])
    print(predictions)
    print(len(predictions))
    print(len(outputs["instances"].pred_classes))
    outputs["instances"].pred_classes=torch.tensor(predictions, device='cuda:0')
    # Filter the detection results for the desired class
    out = v.draw_instance_predictions(outputs["instances"].to("cpu"))
    cv2_imshow(out.get_image()[:, :, ::-1])

kkkk

for box in kkkk:
  print(type(box))

!nvidia-smi

cap = cv2.VideoCapture("/content/drive/MyDrive/Colab Notebooks/Project/video.mp4")
fps = cap.get(cv2.CAP_PROP_FPS)
width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)
height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)

size = (int(width), int(height))
# 设置编码格式、输出路径、fps、size
fourcc = cv2.VideoWriter_fourcc('m', 'p', '4', 'v')
video = cv2.VideoWriter('/content/drive/MyDrive/Colab Notebooks/Project/output.mp4', fourcc, fps, size)  # size可能会与result尺寸不匹配
i=0
orange=(245, 147, 66)
while cap.isOpened():
  ret, frame = cap.read()
  samples=[]
  if ret:
    # outputs = predictor(frame)
    # Resize the frame to the desired size
    resized_frame = cv2.resize(frame, size)

    # Perform object detection on the resized frame
    outputs = predictor(resized_frame)

    cropped,indicies=detect_and_crop(resized_frame,outputs)
    for cropped_img in cropped:
      samples.append(Sample(img=img_transform(cv2.resize(cropped_img,(200,300)), inference_transform)))
    train_feats = [sample.torch_img.numpy() for sample in samples]
    train_feats = torch.tensor(train_feats).to(torch.float32)
    train_x = torch.reshape(train_feats,[train_feats.shape[0],3,384,256])
    # print(train_x.shape)
    result,conv=vgg_model(train_x.cuda().requires_grad_(False))
    torch.cuda.empty_cache()
    predictions = svm_model.predict(conv.detach().cpu().requires_grad_(False))
    # kkkk=outputs["instances"].pred_boxes
    outputs["instances"].set("pred_classes",torch.tensor(predictions))
    outputs["instances"].set("pred_boxes",outputs["instances"].pred_boxes[indicies])
    outputs["instances"].set("scores",outputs["instances"].scores[indicies])
    # Draw bounding boxes and labels on the resized frame
    colors=((255, 0, 0),(0, 255, 0),(0, 0, 255))
    for box, cat in zip(outputs["instances"].pred_boxes.tensor.cpu().numpy(),outputs["instances"].pred_classes.cpu().numpy()
):
        x_min, y_min, x_max, y_max = box.astype(int)
        cv2.rectangle(resized_frame, (x_min, y_min), (x_max, y_max), colors[cat], 2)
    num_ones = torch.sum(outputs["instances"].pred_classes == 1).item()
    num_twos = torch.sum(outputs["instances"].pred_classes == 2).item()
    if(num_twos==0):
        opt_time=num_ones*5
    else:
        opt_time=num_ones*5//num_twos
    # Display the person count in the top-left corner of the resized frame
    cv2.putText(resized_frame, f"queuing People: {num_ones}", (10, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.9, orange, 2)
    cv2.putText(resized_frame, f"WaitingTime: {num_ones*5}s", (10, 40), cv2.FONT_HERSHEY_SIMPLEX, 0.9, orange, 2)
    cv2.putText(resized_frame, f"Num_server: {num_twos}", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.75, orange, 2)
    cv2.putText(resized_frame, f"Optimal WaitingTime: {opt_time}s", (10, 80), cv2.FONT_HERSHEY_SIMPLEX, 0.75, orange, 2)
    i+=1
    if(i%100==0):
      print(i)
    # if(i>=100):
    #   break
    # Resize the processed frame back to the original size for writing to the output video
    frame_with_boxes = cv2.resize(resized_frame, size)
    video.write(frame_with_boxes)  # 写入视频
  else:
    break
cap.release()
video.release()

cap = cv2.VideoCapture("/content/drive/MyDrive/Colab Notebooks/Project/video.mp4")
fps = cap.get(cv2.CAP_PROP_FPS)
width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)
height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)

size = (int(width), int(height))
# 设置编码格式、输出路径、fps、size
fourcc = cv2.VideoWriter_fourcc('m', 'p', '4', 'v')
video = cv2.VideoWriter('/content/drive/MyDrive/Colab Notebooks/Project/output.mp4', fourcc, fps, size)  # size可能会与result尺寸不匹配
i=0
orange=(245, 147, 66)
colors=((0, 0, 255), (255, 0, 0),(0, 255, 0),(0, 0, 255))
while cap.isOpened():
  ret, frame = cap.read()
  samples=[]
  if ret:
    # outputs = predictor(frame)
    # Resize the frame to the desired size
    resized_frame = cv2.resize(frame, size)

    # Perform object detection on the resized frame
    outputs = predictor(resized_frame)

    for box, cat in zip(outputs["instances"].pred_boxes.tensor.cpu().numpy(),outputs["instances"].pred_classes.cpu().numpy()
):
        x_min, y_min, x_max, y_max = box.astype(int)
        cv2.rectangle(resized_frame, (x_min, y_min), (x_max, y_max), colors[cat], 2)
    num_ones = torch.sum(outputs["instances"].pred_classes == 2).item()
    # Display the person count in the top-left corner of the resized frame
    cv2.putText(resized_frame, f"queuing People: {num_ones}", (10, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.9, orange, 2)
    cv2.putText(resized_frame, f"WaitingTime: {num_ones*5}s", (10, 40), cv2.FONT_HERSHEY_SIMPLEX, 0.9, orange, 2)
    i+=1
    if(i%100==0):
      print(i)
    # if(i>=100):
    #   break
    # Resize the processed frame back to the original size for writing to the output video
    frame_with_boxes = cv2.resize(resized_frame, size)
    video.write(frame_with_boxes)  # 写入视频
  else:
    break
cap.release()
video.release()

print("a\nb")

num_ones = torch.sum(outputs["instances"].pred_boxes[indicies].tensor().cpu() == 1).item()

outputs["instances"]

"""# training only with detectron"""

from detectron2.data.datasets import register_coco_instances
register_coco_instances("_train", {}, "/content/drive/MyDrive/Colab Notebooks/Project/Queue counting and detection.v4i.coco/train/_annotations.coco.json", "/content/drive/MyDrive/Colab Notebooks/Project/Queue counting and detection.v4i.coco/train")
register_coco_instances("_valid", {}, "/content/drive/MyDrive/Colab Notebooks/Project/Queue counting and detection.v4i.coco/valid/_annotations.coco.json", "/content/drive/MyDrive/Colab Notebooks/Project/Queue counting and detection.v4i.coco/valid")
train_metadata = MetadataCatalog.get("_train")
train_dataset_dicts = DatasetCatalog.get("_train")
valid_metadata = MetadataCatalog.get("_valid")
valid_dataset_dicts = DatasetCatalog.get("_valid")
# train_metadata.thing_classes.pop(0)
# valid_metadata.thing_classes.pop(0)

train_metadata.thing_classes

from matplotlib import pyplot as plt
# Visualize some random samples
for d in random.sample(train_dataset_dicts, 8):
    img = cv2.imread(d["file_name"])
    visualizer = Visualizer(img[:, :, ::-1], metadata=train_metadata, scale=0.5)
    vis = visualizer.draw_dataset_dict(d)
    plt.axis('off')
    plt.imshow(vis.get_image())
    plt.show()

from detectron2.engine import DefaultTrainer

cfg = get_cfg()
cfg.merge_from_file(model_zoo.get_config_file("COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml"))
cfg.DATASETS.TRAIN = ("_train",)
cfg.DATASETS.TEST = ("_valid",)
cfg.DATALOADER.NUM_WORKERS = 2
cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml")  # Let training initialize from model zoo
cfg.SOLVER.IMS_PER_BATCH = 2  # This is the real "batch size" commonly known to deep learning people
cfg.SOLVER.BASE_LR = 0.0025  # pick a good LR
cfg.SOLVER.MAX_ITER = 600    # 300 iterations seems good enough for this toy dataset; you will need to train longer for a practical dataset
cfg.SOLVER.STEPS = []        # do not decay learning rate
cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # The "RoIHead batch size". 128 is faster, and good enough for this toy dataset (default: 512)
cfg.MODEL.ROI_HEADS.NUM_CLASSES = 3  # only has one class (ballon). (see https://detectron2.readthedocs.io/tutorials/datasets.html#update-the-config-for-new-datasets)
# NOTE: this config means the number of classes, but a few popular unofficial tutorials incorrect uses num_classes+1 here.

os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)
trainer = DefaultTrainer(cfg)
trainer.resume_or_load(resume=False)
trainer.train()

# Commented out IPython magic to ensure Python compatibility.
# Look at training curves in tensorboard:
# %load_ext tensorboard
# %tensorboard --logdir output

# Inference should use the config with parameters that are used in training
# cfg now already contains everything we've set previously. We changed it a little bit for inference:
# cfg = get_cfg()
cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, "model_final.pth")  # path to the model we just trained
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7   # set a custom testing threshold
predictor = DefaultPredictor(cfg)

from detectron2.utils.visualizer import ColorMode
dataset_dicts = valid_dataset_dicts
for d in random.sample(dataset_dicts, 4):
    im = cv2.imread(d["file_name"])
    outputs = predictor(im)  # format is documented at https://detectron2.readthedocs.io/tutorials/models.html#model-output-format
    print(outputs)
    v = Visualizer(im[:, :, ::-1],
                   metadata=valid_metadata,
                   scale=0.5,
                   instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels. This option is only available for segmentation models
    )
    out = v.draw_instance_predictions(outputs["instances"].to("cpu"))
    cv2_imshow(out.get_image()[:, :, ::-1])

outputs

import cv2
import numpy as np

# Load the image
image = cv2.imread('/content/drive/MyDrive/Colab Notebooks/Project/queue-detecting.v1i.coco/train/0a50f752e35a0c0e1dabd6d4f4533cc_jpg.rf.2e6af8e4b3f6b0f4c647fb496a8e37bb.jpg', cv2.IMREAD_GRAYSCALE)
plt.imshow(image)
# Apply Canny edge detection
edges = cv2.Canny(image, 50, 150)

# Perform Hough line detection
lines = cv2.HoughLines(edges, rho=1, theta=np.pi/180, threshold=100)

# Draw detected lines on the image
if lines is not None:
    for line in lines:
        rho, theta = line[0]
        cos_theta = np.cos(theta)
        sin_theta = np.sin(theta)
        x0 = rho * cos_theta
        y0 = rho * sin_theta
        x1 = int(x0 + 1000 * (-sin_theta))
        y1 = int(y0 + 1000 * cos_theta)
        x2 = int(x0 - 1000 * (-sin_theta))
        y2 = int(y0 - 1000 * cos_theta)
        cv2.line(image, (x1, y1), (x2, y2), (0, 0, 255), 2)

# Display the image with detected lines
cv2.imshow('Lines', image)
cv2.waitKey(0)
cv2.destroyAllWindows()

lines.shape

import os
import random
import cv2
import numpy as np
import torch
from detectron2.engine import DefaultTrainer
from detectron2.config import get_cfg
from detectron2.utils.visualizer import Visualizer
from detectron2.data import DatasetCatalog, MetadataCatalog
from detectron2.data.datasets import register_coco_instances
from detectron2 import model_zoo

data_dir = "/content/drive/MyDrive/Colab Notebooks/Project/Queue counting and detection.v8i.coco"  # Replace with the path to your dataset directory

# Register your training and validation datasets
register_coco_instances("_train", {}, "/content/drive/MyDrive/Colab Notebooks/Project/Machine detection.v1i.coco/train/_annotations.coco.json", "/content/drive/MyDrive/Colab Notebooks/Project/Machine detection.v1i.coco/train")
register_coco_instances("_val", {}, "/content/drive/MyDrive/Colab Notebooks/Project/Machine detection.v1i.coco/valid/_annotations.coco.json", "/content/drive/MyDrive/Colab Notebooks/Project/Machine detection.v1i.coco/valid")

train_metadata = MetadataCatalog.get("_train")
train_dataset_dicts = DatasetCatalog.get("_train")
valid_metadata = MetadataCatalog.get("_val")
valid_dataset_dicts = DatasetCatalog.get("_val")
# train_metadata.thing_classes.pop(0)
# valid_metadata.thing_classes.pop(0)

"""# Detect machine in the image"""

train_metadata.thing_classes

from detectron2.engine import DefaultTrainer

cfg = get_cfg()
cfg.merge_from_file(model_zoo.get_config_file("COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml"))
cfg.DATASETS.TRAIN = ("_train",)
cfg.DATASETS.TEST = ("_val",)
cfg.DATALOADER.NUM_WORKERS = 2
cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml")  # Let training initialize from model zoo
cfg.SOLVER.IMS_PER_BATCH = 4  # This is the real "batch size" commonly known to deep learning people
cfg.SOLVER.BASE_LR = 0.00025  # pick a good LR
cfg.SOLVER.MAX_ITER = 600    # 300 iterations seems good enough for this toy dataset; you will need to train longer for a practical dataset
cfg.SOLVER.STEPS = []        # do not decay learning rate
cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # The "RoIHead batch size". 128 is faster, and good enough for this toy dataset (default: 512)
cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # only has one class (ballon). (see https://detectron2.readthedocs.io/tutorials/datasets.html#update-the-config-for-new-datasets)
# NOTE: this config means the number of classes, but a few popular unofficial tutorials incorrect uses num_classes+1 here.

os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)
trainer = DefaultTrainer(cfg)
trainer.resume_or_load(resume=False)
trainer.train()

# Inference should use the config with parameters that are used in training
# cfg now already contains everything we've set previously. We changed it a little bit for inference:
cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, "counting_model.pth")  # path to the model we just trained
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7   # set a custom testing threshold
predictor = DefaultPredictor(cfg)

from detectron2.utils.visualizer import ColorMode
dataset_dicts = train_dataset_dicts
for d in random.sample(dataset_dicts, 4):
    im = cv2.imread(d["file_name"])
    outputs = predictor(im)  # format is documented at https://detectron2.readthedocs.io/tutorials/models.html#model-output-format
    v = Visualizer(im[:, :, ::-1],
                   metadata=train_metadata,
                   scale=0.5,
                   instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels. This option is only available for segmentation models
    )
    out = v.draw_instance_predictions(outputs["instances"].to("cpu"))
    cv2_imshow(out.get_image()[:, :, ::-1])

cfg.OUTPUT_DIR