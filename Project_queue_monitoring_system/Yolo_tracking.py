# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colab.

Original file is located at
  https://colab.research.google.com/drive/11Ek1QqWwqsUtlDkSe10uuxBVUPdadqvD
"""

from google.colab import drive
drive.mount('/content/drive')

# !pip install -r requirements.txt
# !pip install tqdm gradio
!pip install ultralytics
import os
os.chdir('/content/drive/MyDrive/Colab Notebooks/Project/yolov8-deepsort-tracking-main')
import tempfile
from pathlib import Path
import numpy as np
import cv2 # opencv-python
from ultralytics import YOLO

import deep_sort.deep_sort.deep_sort as ds

# Commented out IPython magic to ensure Python compatibility.
# !pip install nbimporter
import os
import importlib
# Import code from other_notebook.ipynb
os.chdir('/content/drive/MyDrive/Colab Notebooks/Project/')
# %run VGG_net.ipynb
# from Classifier import VGGFeature

from sklearn import svm
import joblib
# Load the pretrained model state dictionary
# pretrained_model_path = "/content/drive/MyDrive/Colab Notebooks/Project/output_model/vgg_extractor.pt"
pretrained_model_path = "/content/drive/MyDrive/Colab Notebooks/Project/output_model/3_class_vgg_extractor.pt"
vgg_model = torch.load(pretrained_model_path)
pretrained_model_path = "/content/drive/MyDrive/Colab Notebooks/Project/output_model/3_class_SVM_model.pkl"
svm_model = joblib.load(pretrained_model_path)



import os
import cv2
import matplotlib.pyplot as plt
from PIL import Image
import numpy as np
import torchvision.transforms as transforms

norm_mean = [0.485, 0.456, 0.406]
norm_std = [0.229, 0.224, 0.225]

inference_transform = transforms.Compose([
  transforms.Resize(256),
  transforms.ToTensor(),
  transforms.Normalize(norm_mean, norm_std),
])

def img_transform(img_rgb, transform=None):
  """
  transform images
  :param img_rgb: PIL Image
  :param transform: torchvision.transform
  :return: tensor
  """

  if transform is None:
    raise ValueError("there is no transform")

  img_t = transform(Image.fromarray(img_rgb))
  return img_t
labels={0:'not_queue',1:'queuing',2:'serving'}
cat={'not_queue':0, 'queuing':1, 'serving':2}

import torch
def putTextWithBackground(img, text, origin, font=cv2.FONT_HERSHEY_SIMPLEX, font_scale=1, text_color=(255, 255, 255), bg_color=(0, 0, 0), thickness=1):
  """

  :param img:
  :param text:
  :param origin:
  :param font:
  :param font_scale: 
  :param text_color:
  :param bg_color: 
  :param thickness: 
  """
 
  (text_width, text_height), _ = cv2.getTextSize(text, font, font_scale, thickness)

  bottom_left = origin
  top_right = (origin[0] + text_width, origin[1] - text_height - 5) 
  cv2.rectangle(img, bottom_left, top_right, bg_color, -1)

  text_origin = (origin[0], origin[1] - 5) 
  cv2.putText(img, text, text_origin, font, font_scale, text_color, thickness, lineType=cv2.LINE_AA)

def extract_detections(results, detect_class):
  """

  - results: YoloV8
  - detect_class:
  reference: https://docs.ultralytics.com/modes/predict/#working-with-results
  """

  detections = np.empty((0, 4))

  confarray = []

  for r in results:
    for box in r.boxes:
      if box.cls[0].int() == detect_class:
        x1, y1, x2, y2 = box.xywh[0].int().tolist() 
        conf = round(box.conf[0].item(), 2)
        detections = np.vstack((detections, np.array([x1, y1, x2, y2])))
        confarray.append(conf) 
  return detections, confarray 

def detect_and_track(input_path: str, output_path: str, detect_class: int, model, tracker) -> Path:
  cap = cv2.VideoCapture(input_path) 
  if not cap.isOpened(): 
    print(f"Error opening video file {input_path}")
    return None
  orange=(245, 147, 66)
  fps = cap.get(cv2.CAP_PROP_FPS) 
  size = (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))) 
  output_video_path =output_path+ "output.mp4" 

  fourcc = cv2.VideoWriter_fourcc('m', 'p', '4', 'v')
  # cv2.VideoWriter('/content/drive/MyDrive/Colab Notebooks/Project/output.mp4', fourcc, fps, size)
  output_video = cv2.VideoWriter(output_video_path, fourcc, fps, size, isColor=True) 
  i=0

  while True:
    success, frame = cap.read() 

    if not (success):
      break
    # if(i>20):
    #  break
    results = model(frame, stream=True)
    i+=1
    detections, confarray = extract_detections(results, detect_class)


    resultsTracker = tracker.update(detections, confarray, frame)
    num_ones=0
    num_twos=0
    colors=((255, 0, 0),(0, 255, 0),(0, 0, 255),(255, 0, 255))
    for x1, y1, x2, y2, Id in resultsTracker:
      x1, y1, x2, y2 = map(int, [x1, y1, x2, y2]) 
      cropped_frame = frame[y1:y2, x1:x2]
      # samples.append(Sample(img=img_transform(cv2.resize(cropped_img,(200,300)), inference_transform)))
      train_feat=img_transform(cv2.resize(cropped_frame,(200,300)), inference_transform)
      train_feat = torch.tensor(train_feat).to(torch.float32)
      train_x = torch.reshape(train_feat,[1,3,384,256])
      result,conv=vgg_model(train_x.cuda().requires_grad_(False))
      torch.cuda.empty_cache()
      prediction = svm_model.predict(conv.detach().cpu().requires_grad_(False))
      cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 255), 3)
      text=str(int(Id))+": "+labels[prediction[0]]
      if(prediction[0]==1):
       num_ones+=1
      if(prediction[0]==2):
       num_twos+=1
      putTextWithBackground(frame, text, (max(-10, x1), max(40, y1)), font_scale=1, text_color=(255, 255, 255), bg_color=colors[prediction[0]])
    if(num_twos==0):
     opt_time=num_ones*5
    else:
     opt_time=num_ones*5//num_twos
    cv2.putText(frame, f"queuing People: {num_ones}", (10, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.75, orange, 2)
    cv2.putText(frame, f"WaitingTime: {num_ones*5}s", (10, 40), cv2.FONT_HERSHEY_SIMPLEX, 0.75, orange, 2)
    cv2.putText(frame, f"Num_server: {num_twos}", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.75, orange, 2)
    cv2.putText(frame, f"Optimal WaitingTime: {opt_time}s", (10, 80), cv2.FONT_HERSHEY_SIMPLEX, 0.75, orange, 2)
    output_video.write(frame) 

  output_video.release() 
  cap.release()

  # print(f'output dir is: {output_video_path}')
  return output_video_path

if __name__ == "__main__":
  ######
  input_path = "video.mp4"
  ######

  output_path = "/content/drive/MyDrive/Colab Notebooks/Project/yolov8-deepsort-tracking-main/" 

  model = YOLO("yolov8n.pt")



  detect_class = 0

  tracker = ds.DeepSort("/content/drive/MyDrive/Colab Notebooks/Project/yolov8-deepsort-tracking-main/deep_sort/deep_sort/deep/checkpoint/ckpt.t7")

  detect_and_track(input_path, output_path, detect_class, model, tracker)